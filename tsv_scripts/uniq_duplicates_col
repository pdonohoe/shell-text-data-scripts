#!/bin/sh
# This script is a variation on uniq -d
# Given a delimiter and a column number, it prints out only those lines
# where the value in the column is duplicated in the file
# The lines do not need to be sorted
# Output lines will be in a different order; as this script reads from STDIN, a two-pass operation on a file as suggested in 
# https://unix.stackexchange.com/questions/183428/how-to-run-awk-twice-across-the-same-file cannot be used.
# Thus, to retain the original order of lines, the lines themselves would need to be stored, either in a temporary file,
# or in an awk array. This would be expensive for large files.
# This command can only accept input from STDIN, arguments are ignored
# See https://stackoverflow.com/questions/1450085/list-only-duplicate-lines-based-on-one-column-from-a-semi-colon-delimited-file

# get name of this script
command_name=$(basename "$0");
# get directory of this script
script_path=$(cd "$(dirname "$0")" || exit; pwd);
if [ "$script_path" = "" ] || [ ! -d "$script_path" ] || [ ! -e "$script_path/$command_name" ]; then echo "Variables \$command_name and \$script_path not set correctly for script $command_name"; exit 1; fi;

# command help
help_usage="

Syntax of $command_name:

$command_name -d delimiter -f column

REQUIRED OPTIONS:
-f      Field to uniq on

OPTIONAL OPTIONS:
-d      character used as delimiter (TAB is used as a default)
-l      output only the last line with the unique value in the given column

Examples: 
$command_name -f 6
$command_name -d; -f 3

";

sep="$(printf '\t')";
last_line=0;

# read command options
while getopts "f:d:lh?" getopts_option
do
  case $getopts_option in
  d ) sep="$OPTARG";;	
	f ) field="$OPTARG";;
  l ) last_line=1;; 

	h | \? ) echo "$help_usage"; exit 0;
  esac
done;
# move shell command-line-argument pointer to end of options
shift $((OPTIND - 1));

# validate options
# option -f is required 
[ "$field" = "" ] && echo "Must specify field -f;$help_usage" 1>&2 && exit 0;

if [ "$last_line" -eq 0  ]; then

awk -v col="$field" -v sep="$sep" 'BEGIN { FS = sep } {
    # Keep count of the fields in second column
    count[$col]++;

    # Save the line the first time we encounter a unique field
    if (count[$col] == 1)
        first[$col] = $0;

    # If we encounter the field for the second time, print the
    # previously saved line
    if (count[$col] == 2)
        print first[$col];

    # From the second time onward. always print because the field is
    # duplicated
    if (count[$col] > 1)
        print
}'

else
:
awk -v col="$field" -v sep="$sep" 'BEGIN { FS = sep } 
END { 
    for (i in order) {
      col=order[i];
      if (count[col] > 1) { print lines[col]; }
    }
} {
    # Keep count of the fields in second column
    count[$col]++;
    
    # append to an array of the field in the second column, if it has not already been added
    if (count[$col] == 1) { order[i] = $col; i++; }

    # add / replace hash of lines
    lines[$col] = $0;
}'

fi;
